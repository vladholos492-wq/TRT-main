"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö.
"""

import logging
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
from pathlib import Path
import json
import gzip

logger = logging.getLogger(__name__)

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∞—Ä—Ö–∏–≤–æ–≤
ARCHIVE_DIR = Path("data/archives")
ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)


def archive_old_generations(days_to_keep: int = 90, batch_size: int = 1000) -> Dict[str, Any]:
    """
    –ê—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç —Å—Ç–∞—Ä—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª—ã.
    
    Args:
        days_to_keep: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
    """
    try:
        from database import get_db_connection
        
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
                cur.execute("""
                    SELECT COUNT(*) 
                    FROM generations 
                    WHERE created_at < %s
                """, (cutoff_date,))
                total_count = cur.fetchone()[0]
                
                if total_count == 0:
                    return {
                        'archived': 0,
                        'deleted': 0,
                        'files_created': 0
                    }
                
                # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏–≤–Ω—ã–π —Ñ–∞–π–ª
                archive_file = ARCHIVE_DIR / f"generations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json.gz"
                
                archived_count = 0
                deleted_count = 0
                
                # –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º –±–∞—Ç—á–∞–º–∏
                offset = 0
                while True:
                    cur.execute("""
                        SELECT id, user_id, model_id, model_name, params, result_urls, 
                               task_id, price, is_free, created_at
                        FROM generations 
                        WHERE created_at < %s
                        ORDER BY created_at
                        LIMIT %s OFFSET %s
                    """, (cutoff_date, batch_size, offset))
                    
                    batch = cur.fetchall()
                    if not batch:
                        break
                    
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –∞—Ä—Ö–∏–≤
                    archive_data = []
                    for row in batch:
                        archive_data.append({
                            'id': row[0],
                            'user_id': row[1],
                            'model_id': row[2],
                            'model_name': row[3],
                            'params': row[4] if isinstance(row[4], dict) else json.loads(row[4]) if row[4] else {},
                            'result_urls': row[5] if isinstance(row[5], list) else json.loads(row[5]) if row[5] else [],
                            'task_id': row[6],
                            'price': float(row[7]) if row[7] else 0,
                            'is_free': row[8],
                            'created_at': row[9].isoformat() if row[9] else None
                        })
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∞—Ä—Ö–∏–≤–Ω—ã–π —Ñ–∞–π–ª
                    with gzip.open(archive_file, 'at', encoding='utf-8') as f:
                        for item in archive_data:
                            f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    
                    # –£–¥–∞–ª—è–µ–º –∏–∑ –ë–î
                    ids_to_delete = [row[0] for row in batch]
                    cur.execute("""
                        DELETE FROM generations 
                        WHERE id = ANY(%s)
                    """, (ids_to_delete,))
                    
                    archived_count += len(batch)
                    deleted_count += len(ids_to_delete)
                    offset += batch_size
                    
                    logger.info(f"üì¶ –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ {archived_count}/{total_count} –∑–∞–ø–∏—Å–µ–π...")
                
                conn.commit()
                
                logger.info(f"‚úÖ –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ {archived_count} –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –≤ {archive_file}")
                
                return {
                    'archived': archived_count,
                    'deleted': deleted_count,
                    'files_created': 1,
                    'archive_file': str(archive_file)
                }
                
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π: {e}", exc_info=True)
        return {
            'archived': 0,
            'deleted': 0,
            'files_created': 0,
            'error': str(e)
        }


def archive_old_sessions(days_to_keep: int = 7) -> Dict[str, Any]:
    """
    –ê—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç —Å—Ç–∞—Ä—ã–µ —Å–µ—Å—Å–∏–∏.
    
    Args:
        days_to_keep: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
    """
    try:
        from database import get_db_connection
        
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
                cur.execute("""
                    SELECT COUNT(*) 
                    FROM operations 
                    WHERE type = 'session' AND created_at < %s
                """, (cutoff_date,))
                total_count = cur.fetchone()[0]
                
                if total_count == 0:
                    return {
                        'archived': 0,
                        'deleted': 0
                    }
                
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–µ—Å—Å–∏–∏ (–æ–Ω–∏ –æ–±—ã—á–Ω–æ –Ω–µ —Ç—Ä–µ–±—É—é—Ç –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏)
                cur.execute("""
                    DELETE FROM operations 
                    WHERE type = 'session' AND created_at < %s
                """, (cutoff_date,))
                
                deleted_count = cur.rowcount
                conn.commit()
                
                logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä—ã—Ö —Å–µ—Å—Å–∏–π")
                
                return {
                    'archived': 0,
                    'deleted': deleted_count
                }
                
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–π: {e}", exc_info=True)
        return {
            'archived': 0,
            'deleted': 0,
            'error': str(e)
        }


def cleanup_old_archives(days_to_keep: int = 365) -> int:
    """
    –£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –∞—Ä—Ö–∏–≤–Ω—ã–µ —Ñ–∞–π–ª—ã.
    
    Args:
        days_to_keep: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∞—Ä—Ö–∏–≤–æ–≤
    
    Returns:
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    """
    try:
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        deleted_count = 0
        
        for archive_file in ARCHIVE_DIR.glob("*.json.gz"):
            file_time = datetime.fromtimestamp(archive_file.stat().st_mtime)
            if file_time < cutoff_date:
                archive_file.unlink()
                deleted_count += 1
                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –∞—Ä—Ö–∏–≤: {archive_file.name}")
        
        return deleted_count
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∞—Ä—Ö–∏–≤–æ–≤: {e}", exc_info=True)
        return 0

