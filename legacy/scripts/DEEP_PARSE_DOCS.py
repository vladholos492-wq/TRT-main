#!/usr/bin/env python3
"""
üéØ DEEP PARSER - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• –¥–µ—Ç–∞–ª–µ–π –∏–∑ Kie.ai docs

–ü–∞—Ä—Å–∏—Ç –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —á—Ç–æ–±—ã –∏–∑–≤–ª–µ—á—å:
1. Model IDs –∏ –∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç—ã
2. Request/Response –ø—Ä–∏–º–µ—Ä—ã
3. –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å —Ç–∏–ø–∞–º–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏
4. Pricing (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
5. –≠–Ω–¥–ø–æ–π–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏

–¶–ï–õ–¨: –°–æ–∑–¥–∞—Ç—å –ü–û–õ–ù–´–ô SOURCE OF TRUTH –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
"""
import json
import re
from pathlib import Path
from bs4 import BeautifulSoup
from typing import Dict, List, Any
from datetime import datetime


def extract_code_examples(html: str) -> List[Dict]:
    """–ò–∑–≤–ª–µ—á—å –≤—Å–µ code examples –∏–∑ HTML"""
    soup = BeautifulSoup(html, 'lxml')
    examples = []
    
    # –ò—â–µ–º –≤—Å–µ code blocks
    code_blocks = soup.find_all(['code', 'pre'])
    
    for block in code_blocks:
        text = block.get_text()
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON
        json_matches = re.finditer(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
        
        for match in json_matches:
            json_text = match.group(0)
            try:
                data = json.loads(json_text)
                examples.append(data)
            except:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –æ—á–∏—Å—Ç–∏—Ç—å –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å
                try:
                    cleaned = re.sub(r'//.*\n', '\n', json_text)  # Remove comments
                    cleaned = re.sub(r',\s*}', '}', cleaned)  # Remove trailing commas
                    data = json.loads(cleaned)
                    examples.append(data)
                except:
                    pass
    
    return examples


def parse_api_page_deep(html_file: Path) -> Dict[str, Any]:
    """–ì–ª—É–±–æ–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã API"""
    html = html_file.read_text(encoding='utf-8')
    soup = BeautifulSoup(html, 'lxml')
    
    info = {
        "file": html_file.name,
        "title": "",
        "endpoints": [],
        "models": set(),
        "parameters": {},
        "examples": {},
        "pricing": {}
    }
    
    # Title
    title = soup.find('h1')
    if title:
        info['title'] = title.get_text(strip=True)
    
    # Extract all URLs
    urls = re.findall(r'https://api\.kie\.ai/[^\s"\'<>]+', html)
    info['endpoints'] = list(set(urls))
    
    # Extract code examples
    examples = extract_code_examples(html)
    
    # Analyze examples
    for ex in examples:
        # Model ID
        if 'model' in ex:
            info['models'].add(ex['model'])
        
        # Parameters from examples
        if 'input' in ex or isinstance(ex, dict):
            params = ex.get('input', ex)
            for key, value in params.items():
                if key not in info['parameters']:
                    info['parameters'][key] = {
                        'examples': [],
                        'type': type(value).__name__
                    }
                info['parameters'][key]['examples'].append(value)
        
        # Store full example
        example_type = 'request' if any(k in ex for k in ['model', 'input', 'prompt']) else 'response'
        if example_type not in info['examples']:
            info['examples'][example_type] = []
        info['examples'][example_type].append(ex)
    
    # Convert sets to lists for JSON serialization
    info['models'] = sorted(list(info['models']))
    
    return info


def main():
    docs_dir = Path('cache/kie_docs')
    
    if not docs_dir.exists():
        print("‚ùå Cache dir not found. Run PARSE_KIE_DOCS.py first")
        return
    
    all_apis = {}
    
    print("="*80)
    print("üéØ DEEP PARSING KIE.AI DOCS")
    print("="*80)
    
    for html_file in sorted(docs_dir.glob('*.html')):
        print(f"\nüìÑ Parsing: {html_file.name}")
        
        api_info = parse_api_page_deep(html_file)
        
        if api_info['endpoints'] or api_info['models']:
            all_apis[html_file.stem] = api_info
            
            print(f"   Title: {api_info['title']}")
            print(f"   Endpoints: {len(api_info['endpoints'])}")
            print(f"   Models: {api_info['models']}")
            print(f"   Parameters: {len(api_info['parameters'])}")
            print(f"   Examples: request={len(api_info['examples'].get('request', []))}, response={len(api_info['examples'].get('response', []))}")
    
    # Save results
    output = {
        "version": "DEEP_PARSE_1.0",
        "parsed_at": datetime.now().isoformat(),
        "source": "docs.kie.ai (deep parse)",
        "apis": all_apis,
        "summary": {
            "total_apis": len(all_apis),
            "total_endpoints": sum(len(api['endpoints']) for api in all_apis.values()),
            "total_models": len(set(m for api in all_apis.values() for m in api['models'])),
            "total_params": sum(len(api['parameters']) for api in all_apis.values())
        }
    }
    
    output_file = Path("models/kie_docs_deep_parse.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*80}")
    print(f"üíæ Saved: {output_file}")
    print(f"\nüìä Summary:")
    for key, value in output['summary'].items():
        print(f"   {key}: {value}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
    all_models = sorted(set(m for api in all_apis.values() for m in api['models']))
    if all_models:
        print(f"\nüéØ –ù–∞–π–¥–µ–Ω–Ω—ã–µ Model IDs ({len(all_models)}):")
        for model in all_models:
            print(f"   - {model}")


if __name__ == "__main__":
    main()
