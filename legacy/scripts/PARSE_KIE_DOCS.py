#!/usr/bin/env python3
"""
üéØ KIE.AI DOCS PARSER - SINGLE SOURCE OF TRUTH

–ü–∞—Ä—Å–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é Kie.ai —á—Ç–æ–±—ã –∏–∑–≤–ª–µ—á—å:
1. –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ API endpoints –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
2. Model IDs –∏ –∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
3. –ü—Ä–∏–º–µ—Ä—ã request/response
4. Pricing –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

–§–ò–õ–û–°–û–§–ò–Ø:
- Kie.ai docs - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã
- –ü–∞—Ä—Å–∏–º –û–î–ò–ù –†–ê–ó, —Ñ–∏–∫—Å–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
- –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –ø–∞—Ä—Å–∏–Ω–≥—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
"""
import httpx
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup
from datetime import datetime

DOCS_BASE = "https://docs.kie.ai"
CACHE_DIR = Path("cache/kie_docs")
CACHE_DIR.mkdir(parents=True, exist_ok=True)


class KieDocsParser:
    """Parser –¥–ª—è Kie.ai –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.api_categories = {}
        self.models = []
        self.endpoints = {}
        
    def fetch_page(self, path: str) -> Optional[str]:
        """Fetch –∏ cache —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        cache_file = CACHE_DIR / f"{path.replace('/', '_')}.html"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º cache
        if cache_file.exists():
            print(f"üì¶ Cache hit: {path}")
            return cache_file.read_text(encoding='utf-8')
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º
        url = f"{DOCS_BASE}{path}" if path.startswith('/') else f"{DOCS_BASE}/{path}"
        print(f"üåê Fetching: {url}")
        
        try:
            response = httpx.get(url, timeout=30.0, follow_redirects=True)
            if response.status_code == 200:
                cache_file.write_text(response.text, encoding='utf-8')
                return response.text
            else:
                print(f"‚ùå HTTP {response.status_code}")
                return None
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return None
    
    def parse_homepage(self):
        """–ü–∞—Ä—Å–∏–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ API"""
        html = self.fetch_page('/')
        if not html:
            return
        
        soup = BeautifulSoup(html, 'lxml')
        
        # –ò—â–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        categories = {
            "video": [],
            "audio": [],
            "image": [],
            "utility": []
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ API
        for link in soup.find_all('a', href=True):
            href = link['href']
            text = link.get_text(strip=True)
            
            # Video APIs
            if 'veo' in href.lower() or 'runway' in href.lower() or 'video' in text.lower():
                categories['video'].append({'text': text, 'href': href})
            # Audio APIs
            elif 'suno' in href.lower() or 'audio' in text.lower() or 'music' in text.lower():
                categories['audio'].append({'text': text, 'href': href})
            # Image APIs
            elif 'flux' in href.lower() or '4o-image' in href.lower() or 'image' in text.lower():
                categories['image'].append({'text': text, 'href': href})
            # Utility
            elif 'file-upload' in href.lower() or 'common' in href.lower():
                categories['utility'].append({'text': text, 'href': href})
        
        self.api_categories = categories
        
        print("\nüìÇ –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ API:")
        for cat, links in categories.items():
            print(f"\n{cat.upper()} ({len(links)} APIs):")
            for item in links[:5]:
                print(f"  - {item['text']}: {item['href']}")
    
    def parse_api_page(self, path: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ API"""
        html = self.fetch_page(path)
        if not html:
            return {}
        
        soup = BeautifulSoup(html, 'lxml')
        
        api_info = {
            "path": path,
            "endpoints": [],
            "models": [],
            "examples": []
        }
        
        # –ò—â–µ–º code blocks —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
        code_blocks = soup.find_all(['pre', 'code'])
        for block in code_blocks:
            text = block.get_text()
            
            # Endpoint URLs
            endpoint_match = re.search(r'(https://api\.kie\.ai[^\s"\']+)', text)
            if endpoint_match:
                endpoint = endpoint_match.group(1)
                if endpoint not in api_info['endpoints']:
                    api_info['endpoints'].append(endpoint)
            
            # Model IDs –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö
            model_match = re.search(r'"model"\s*:\s*"([^"]+)"', text)
            if model_match:
                model_id = model_match.group(1)
                if model_id not in api_info['models']:
                    api_info['models'].append(model_id)
            
            # JSON –ø—Ä–∏–º–µ—Ä—ã
            if '{' in text and '"input"' in text:
                try:
                    json_match = re.search(r'\{.*\}', text, re.DOTALL)
                    if json_match:
                        example = json.loads(json_match.group(0))
                        api_info['examples'].append(example)
                except:
                    pass
        
        return api_info
    
    def parse_all_categories(self):
        """–ü–∞—Ä—Å–∏–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ API"""
        results = {}
        
        for category, links in self.api_categories.items():
            print(f"\nüîç Parsing {category.upper()} APIs...")
            results[category] = []
            
            for item in links[:3]:  # –ü–µ—Ä–≤—ã–µ 3 –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                href = item['href']
                if not href.startswith('http'):
                    href = href if href.startswith('/') else f'/{href}'
                
                api_info = self.parse_api_page(href)
                if api_info and api_info.get('endpoints'):
                    results[category].append({
                        'name': item['text'],
                        'href': href,
                        **api_info
                    })
        
        return results
    
    def save_results(self, results: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        output = {
            "version": "PARSED_FROM_DOCS_1.0",
            "source": "docs.kie.ai",
            "parsed_at": datetime.now().isoformat(),
            "api_structure": results,
            "summary": {
                "total_categories": len(results),
                "total_apis": sum(len(apis) for apis in results.values()),
                "total_endpoints": sum(
                    len(api.get('endpoints', [])) 
                    for apis in results.values() 
                    for api in apis
                ),
                "total_models": sum(
                    len(api.get('models', [])) 
                    for apis in results.values() 
                    for api in apis
                )
            }
        }
        
        output_file = Path("models/kie_api_structure.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Saved: {output_file}")
        print(f"üìä Summary:")
        for key, value in output['summary'].items():
            print(f"   {key}: {value}")


def main():
    """Main entry point"""
    print("="*80)
    print("üéØ KIE.AI DOCS PARSER - STARTING")
    print("="*80)
    
    parser = KieDocsParser()
    
    # Step 1: Parse homepage
    print("\nüìñ Step 1: Parsing homepage...")
    parser.parse_homepage()
    
    # Step 2: Parse all API categories
    print("\nüìñ Step 2: Parsing API categories...")
    results = parser.parse_all_categories()
    
    # Step 3: Save results
    print("\nüìñ Step 3: Saving results...")
    parser.save_results(results)
    
    print("\n‚úÖ DONE!")


if __name__ == "__main__":
    main()
