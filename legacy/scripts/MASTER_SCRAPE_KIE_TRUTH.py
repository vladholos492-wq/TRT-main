#!/usr/bin/env python3
"""
üéØ MASTER SCRAPER - –ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô SOURCE OF TRUTH

–≠–¢–û –ì–õ–ê–í–ù–´–ô –°–ö–†–ò–ü–¢ –ü–†–û–ï–ö–¢–ê!
–ü–∞—Ä—Å–∏—Ç Kie.ai –∏ —Å–æ–∑–¥–∞–µ—Ç –û–ö–û–ù–ß–ê–¢–ï–õ–¨–ù–´–ô registry –º–æ–¥–µ–ª–µ–π.

–§–ò–õ–û–°–û–§–ò–Ø:
1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–æ–∫ –í–°–ï–• –º–æ–¥–µ–ª–µ–π –∏–∑ docs.kie.ai (JSON —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
2. –î–ª—è –ö–ê–ñ–î–û–ô –º–æ–¥–µ–ª–∏ - –ø–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É + Copy page/API –ø—Ä–∏–º–µ—Ä—ã
3. –ò–∑–≤–ª–µ–∫–∞–µ–º pricing –∏–∑ API/—Å—Ç—Ä–∞–Ω–∏—Ü—ã
4. –°–æ–∑–¥–∞–µ–º –ï–î–ò–ù–´–ô registry —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ models/KIE_SOURCE_OF_TRUTH.json

–≠–¢–û–¢ –§–ê–ô–õ –ó–ê–ü–£–°–ö–ê–ï–¢–°–Ø –û–î–ò–ù –†–ê–ó –ò –°–û–ó–î–ê–ï–¢ –ë–ê–ó–£.
–í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –ø–∞—Ä—Å–∏–Ω–≥—É –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç.
"""

import re
import json
import httpx
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from bs4 import BeautifulSoup
import time


class MasterKieScraper:
    """–ú–∞—Å—Ç–µ—Ä-–ø–∞—Ä—Å–µ—Ä Kie.ai - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∞–≤–¥—ã"""
    
    def __init__(self):
        self.client = httpx.Client(
            timeout=30.0,
            follow_redirects=True,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
        )
        self.cache_dir = Path("cache/kie_model_pages")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.models_registry = {}
        self.pending_models = []
        
    def extract_docs_model_list(self) -> List[Dict[str, str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ docs.kie.ai
        –ò—Å–ø–æ–ª—å–∑—É—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON, –∫–æ—Ç–æ—Ä–∞—è —Ç–∞–º –µ—Å—Ç—å
        """
        
        print("=" * 80)
        print("üìö STEP 1: Extracting model list from docs.kie.ai")
        print("=" * 80)
        
        # –ß–∏—Ç–∞–µ–º –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π docs HTML
        docs_file = Path("cache/kie_docs/_common-api_get-account-credits.html")
        if not docs_file.exists():
            print("‚ùå Docs cache not found, downloading...")
            try:
                resp = self.client.get("https://docs.kie.ai/")
                docs_file.write_text(resp.text, encoding='utf-8')
            except Exception as e:
                print(f"‚ùå Error downloading docs: {e}")
                return []
        
        html = docs_file.read_text(encoding='utf-8')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ Next.js data
        # –ü–∞—Ç—Ç–µ—Ä–Ω: "pages":[{"group":"Seedream","pages":[...]}]
        
        models = []
        
        # –ü–∞—Ä—Å–∏–º market/* –º–æ–¥–µ–ª–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π grep-like –ø–æ–∏—Å–∫
        market_pattern = r'market/([a-zA-Z0-9_-]+)/([a-zA-Z0-9_-]+)'
        market_matches = re.findall(market_pattern, html)
        
        print(f"\nüìã Found {len(market_matches)} market model references")
        
        seen = set()
        for provider, model_slug in market_matches:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ
            if model_slug == 'get-task-detail' or provider == 'common':
                continue
                
            model_id = f"{provider}/{model_slug}"
            path = f"market/{provider}/{model_slug}"
            
            if model_id not in seen:
                seen.add(model_id)
                
                models.append({
                    "model_id": model_id,
                    "provider": provider,
                    "slug": path,
                    "category": self._infer_category(provider, model_slug)
                })
        
        # –î–æ–±–∞–≤–ª—è–µ–º Veo3, Suno, 4o, Flux –∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö API
        special_models = [
            {
                "model_id": "veo3/quality",
                "provider": "google",
                "slug": "veo3-api/generate-veo-3-video",
                "category": "video"
            },
            {
                "model_id": "veo3/fast",
                "provider": "google",
                "slug": "veo3-api/generate-veo-3-video",
                "category": "video"
            },
            {
                "model_id": "suno/v4",
                "provider": "suno",
                "slug": "suno-api/generate-music",
                "category": "audio"
            },
            {
                "model_id": "4o-image/standard",
                "provider": "openai",
                "slug": "4o-image-api/create-image",
                "category": "image"
            },
            {
                "model_id": "flux-kontext/pro",
                "provider": "black-forest-labs",
                "slug": "flux-kontext-api/generate-image",
                "category": "image"
            },
        ]
        
        models.extend(special_models)
        
        print(f"‚úÖ Total models extracted: {len(models)}")
        print(f"   - Providers: {len(set(m['provider'] for m in models))}")
        print(f"   - Categories: {set(m['category'] for m in models)}")
        
        return models
    
    def _infer_category(self, provider: str, model_id: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –º–æ–¥–µ–ª–∏"""
        
        text = f"{provider} {model_id}".lower()
        
        if any(x in text for x in ['image', 'picture', 'photo', 'seedream', 'flux', '4o-image']):
            return 'image'
        elif any(x in text for x in ['video', 'veo', 'runway', 'kling', 'wan', 'luma', 'minimax']):
            return 'video'
        elif any(x in text for x in ['audio', 'music', 'suno', 'sound', 'speech', 'elevenlabs']):
            return 'audio'
        elif any(x in text for x in ['upscale', 'enhance']):
            return 'enhance'
        else:
            return 'other'
    
    def scrape_model_details(self, model_info: Dict) -> Optional[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç:
        - endpoint
        - input_schema (–∏–∑ Copy page / API –ø—Ä–∏–º–µ—Ä–æ–≤)
        - pricing (credits/gen, usd/gen)
        - –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        """
        
        model_id = model_info['model_id']
        slug = model_info['slug']
        
        print(f"\nüìÑ [{model_id}] Scraping docs page...")
        
        # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ docs
        url = f"https://docs.kie.ai/{slug}"
        cache_file = self.cache_dir / f"{slug.replace('/', '_')}.html"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if cache_file.exists():
            print(f"   üì¶ Using cache")
            html = cache_file.read_text(encoding='utf-8')
        else:
            try:
                print(f"   üåê Fetching: {url}")
                resp = self.client.get(url)
                
                if resp.status_code == 404:
                    print(f"   ‚ö†Ô∏è  404 Not Found")
                    return None
                
                html = resp.text
                cache_file.write_text(html, encoding='utf-8')
                time.sleep(0.5)  # Be polite
                
            except Exception as e:
                print(f"   ‚ùå Error: {e}")
                return None
        
        # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
        return self._extract_from_docs_page(html, model_info)
    
    def _extract_from_docs_page(self, html: str, model_info: Dict) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        
        soup = BeautifulSoup(html, 'lxml')
        
        model_data = {
            "model_id": model_info['model_id'],
            "provider": model_info['provider'],
            "category": model_info['category'],
            "slug": model_info['slug'],
            "display_name": None,
            "description": None,
            "endpoint": None,
            "method": "POST",
            "input_schema": {},
            "pricing": {},
            "examples": [],
            "source_url": f"https://docs.kie.ai/{model_info['slug']}"
        }
        
        # 1. Display name (–∏–∑ h1)
        h1 = soup.find('h1')
        if h1:
            model_data['display_name'] = h1.get_text(strip=True)
        else:
            model_data['display_name'] = model_info['model_id']
        
        # 2. Description
        desc_tag = soup.find('p', class_=lambda x: x and ('description' in x.lower() or 'subtitle' in x.lower()) if x else False)
        if desc_tag:
            model_data['description'] = desc_tag.get_text(strip=True)
        else:
            # –ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            first_p = soup.find('p')
            if first_p:
                model_data['description'] = first_p.get_text(strip=True)[:200]
        
        # 3. Endpoint (–∏–∑ code blocks –∏–ª–∏ cURL examples)
        endpoints = self._extract_endpoints(soup)
        if endpoints:
            model_data['endpoint'] = endpoints[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π
            print(f"   ‚úÖ Endpoint: {model_data['endpoint']}")
        
        # 4. Input schema (–∏–∑ JSON –ø—Ä–∏–º–µ—Ä–æ–≤)
        schema, examples = self._extract_input_schema(soup)
        if schema:
            model_data['input_schema'] = schema
            model_data['examples'] = examples
            print(f"   ‚úÖ Schema params: {list(schema.keys())}")
        
        # 5. Pricing (–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–∞–±–ª–∏—Ü)
        pricing = self._extract_pricing(soup, html)
        if pricing:
            model_data['pricing'] = pricing
            print(f"   üí∞ Pricing: {pricing}")
        
        return model_data
    
    def _extract_endpoints(self, soup: BeautifulSoup) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç API endpoints –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        
        endpoints = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω 1: –≤ code blocks
        code_blocks = soup.find_all(['code', 'pre'])
        for block in code_blocks:
            text = block.get_text()
            
            # –ò—â–µ–º https://api.kie.ai/...
            matches = re.findall(r'https://api\.kie\.ai(/api/v\d+/[^\s"\'\)\]]+)', text)
            endpoints.extend(matches)
            
            # –ò—â–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ /api/v1/...
            rel_matches = re.findall(r'(/api/v\d+/jobs/\w+)', text)
            endpoints.extend(rel_matches)
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        return list(dict.fromkeys(endpoints))
    
    def _extract_input_schema(self, soup: BeautifulSoup) -> tuple[Dict, List]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç input schema –∏–∑ JSON –ø—Ä–∏–º–µ—Ä–æ–≤"""
        
        schema = {}
        examples = []
        
        code_blocks = soup.find_all(['code', 'pre'])
        
        for block in code_blocks:
            text = block.get_text()
            
            # –ò—â–µ–º JSON –æ–±—ä–µ–∫—Ç—ã
            try:
                # –ü–∞—Ç—Ç–µ—Ä–Ω: { ... "prompt": ... }
                json_match = re.search(r'\{[^\{\}]*(?:\{[^\{\}]*\}[^\{\}]*)*\}', text, re.DOTALL)
                if json_match:
                    json_obj = json.loads(json_match.group(0))
                    
                    # –≠—Ç–æ request payload?
                    if 'prompt' in json_obj or 'model' in json_obj or 'text' in json_obj:
                        examples.append(json_obj)
                        
                        # –°—Ç—Ä–æ–∏–º schema
                        for key, value in json_obj.items():
                            if key not in schema:
                                schema[key] = {
                                    "type": type(value).__name__,
                                    "required": True,
                                    "examples": []
                                }
                            schema[key]['examples'].append(value)
            except:
                pass
        
        return schema, examples
    
    def _extract_pricing(self, soup: BeautifulSoup, html: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç pricing –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        pricing = {}
        
        text = soup.get_text()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω 1: "X credits per generation"
        credits_match = re.search(r'(\d+(?:\.\d+)?)\s*credits?\s*(?:per|/)\s*(?:gen|generation)', text, re.IGNORECASE)
        if credits_match:
            pricing['credits_per_gen'] = float(credits_match.group(1))
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω 2: "$X per generation" –∏–ª–∏ "$X/gen"
        usd_match = re.search(r'\$(\d+(?:\.\d+)?)\s*(?:per|/)\s*(?:gen|generation)', text, re.IGNORECASE)
        if usd_match:
            pricing['usd_per_gen'] = float(usd_match.group(1))
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω 3: –ø—Ä–æ—Å—Ç–æ "$X" —Ä—è–¥–æ–º —Å "price" –∏–ª–∏ "cost"
        price_match = re.search(r'(?:price|cost)[^\$]*\$(\d+(?:\.\d+)?)', text, re.IGNORECASE)
        if price_match and 'usd_per_gen' not in pricing:
            pricing['usd_per_gen'] = float(price_match.group(1))
        
        return pricing
    
    def build_master_registry(self) -> Dict:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥: —Å—Ç—Ä–æ–∏—Ç –ø–æ–ª–Ω—ã–π registry
        """
        
        print("\n" + "=" * 80)
        print("üéØ BUILDING MASTER SOURCE OF TRUTH REGISTRY")
        print("=" * 80)
        
        # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        model_list = self.extract_docs_model_list()
        
        if not model_list:
            print("‚ùå No models found!")
            return {}
        
        # –®–∞–≥ 2: –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
        print(f"\nüì¶ STEP 2: Scraping {len(model_list)} models")
        print("=" * 80)
        
        for idx, model_info in enumerate(model_list, 1):
            model_id = model_info['model_id']
            print(f"\n[{idx}/{len(model_list)}] Processing: {model_id}")
            
            model_data = self.scrape_model_details(model_info)
            
            if model_data:
                # –í–∞–ª–∏–¥–∞—Ü–∏—è
                if not model_data.get('endpoint'):
                    print(f"   ‚ö†Ô∏è  No endpoint found - adding to pending")
                    self.pending_models.append({
                        "model_id": model_id,
                        "reason": "No endpoint found",
                        **model_info
                    })
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ registry
                    self.models_registry[model_id] = model_data
                    print(f"   ‚úÖ Added to registry")
            else:
                self.pending_models.append({
                    "model_id": model_id,
                    "reason": "Failed to scrape",
                    **model_info
                })
        
        # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω—è–µ–º
        return self._save_registry()
    
    def _save_registry(self) -> Dict:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç registry –≤ JSON"""
        
        registry = {
            "version": "1.0.0-MASTER-SOURCE-OF-TRUTH",
            "scraped_at": datetime.now().isoformat(),
            "source": "docs.kie.ai + page scraping",
            "philosophy": """
–ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô SOURCE OF TRUTH - docs.kie.ai + —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–æ–¥–µ–ª–µ–π.
–°–ø–∞—Ä—Å–µ–Ω–æ –û–î–ò–ù –†–ê–ó —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.
–í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –ø–∞—Ä—Å–∏–Ω–≥—É –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç.
            """,
            "total_models": len(self.models_registry),
            "pending_models": len(self.pending_models),
            "models": self.models_registry,
            "pending": self.pending_models
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        output_file = Path("models/KIE_SOURCE_OF_TRUTH.json")
        output_file.write_text(json.dumps(registry, indent=2, ensure_ascii=False), encoding='utf-8')
        
        print("\n" + "=" * 80)
        print("‚úÖ MASTER REGISTRY CREATED")
        print("=" * 80)
        print(f"üìÑ Saved to: {output_file}")
        print(f"‚úÖ Total models: {len(self.models_registry)}")
        print(f"‚è≥ Pending models: {len(self.pending_models)}")
        
        if self.models_registry:
            print(f"\nüìä Registry stats:")
            categories = {}
            with_pricing = 0
            with_schema = 0
            
            for model_id, data in self.models_registry.items():
                cat = data.get('category', 'unknown')
                categories[cat] = categories.get(cat, 0) + 1
                if data.get('pricing'):
                    with_pricing += 1
                if data.get('input_schema'):
                    with_schema += 1
            
            print(f"   - Categories: {categories}")
            print(f"   - With pricing: {with_pricing}/{len(self.models_registry)}")
            print(f"   - With schema: {with_schema}/{len(self.models_registry)}")
        
        return registry


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    scraper = MasterKieScraper()
    registry = scraper.build_master_registry()
    
    print("\n" + "=" * 80)
    print("üéâ DONE! Source of Truth created.")
    print("=" * 80)
    print("\nNext steps:")
    print("1. Review models/KIE_SOURCE_OF_TRUTH.json")
    print("2. Add manual pricing for pending models if needed")
    print("3. Integrate registry into bot")
    

if __name__ == "__main__":
    main()
