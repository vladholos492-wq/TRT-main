#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä—â–∏–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π Kie.ai.
–ü–∞—Ä—Å–∏—Ç kie.ai/pricing –∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–ª—É—á–∞–µ—Ç API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é.
"""
import requests
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import urljoin

BASE_URL = "https://kie.ai"
PRICING_URL = f"{BASE_URL}/pricing"
USD_TO_RUB = 95.0
MARKUP_PERCENT = 50

def fetch_pricing_page():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É pricing."""
    print(f"üì° Fetching {PRICING_URL}...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    response = requests.get(PRICING_URL, headers=headers)
    return response.text

def extract_model_links(html):
    """–ò–∑–≤–ª–µ—á—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤—Å–µ –º–æ–¥–µ–ª–∏."""
    soup = BeautifulSoup(html, 'html.parser')
    
    # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã: /model-name, /provider/model-name
    model_links = set()
    
    for link in soup.find_all('a', href=True):
        href = link['href']
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        if any(skip in href for skip in ['/pricing', '/api-key', '/login', '/signup', '#']):
            continue
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏
        if href.startswith('/') and len(href) > 1:
            model_links.add(href)
    
    return list(model_links)

def fetch_model_page(model_path):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏."""
    url = urljoin(BASE_URL, model_path)
    print(f"  üìÑ Fetching {url}...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            return response.text
        else:
            print(f"  ‚ö†Ô∏è Status {response.status_code}")
            return None
    except Exception as e:
        print(f"  ‚ùå Error: {e}")
        return None

def extract_pricing_from_table(html):
    """–ò–∑–≤–ª–µ—á—å pricing –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ pricing."""
    soup = BeautifulSoup(html, 'html.parser')
    
    models = []
    
    # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —Ü–µ–Ω–∞–º–∏
    # –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ pricing –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–∞–±–ª–∏—Ü–∞ —Å –º–æ–¥–µ–ª—è–º–∏
    rows = soup.find_all('tr')
    
    for row in rows:
        cells = row.find_all(['td', 'th'])
        if len(cells) >= 3:  # –ú–∏–Ω–∏–º—É–º: model, credits, price
            try:
                # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ
                model_cell = cells[0].get_text(strip=True)
                
                # –ò—â–µ–º credits –∏ price
                credits_text = None
                price_text = None
                
                for cell in cells:
                    text = cell.get_text(strip=True)
                    # –ò—â–µ–º —á–∏—Å–ª–∞ –¥–ª—è credits
                    if re.search(r'\d+\.?\d*\s*(credits?|Credits)', text, re.IGNORECASE):
                        credits_match = re.search(r'(\d+\.?\d*)', text)
                        if credits_match:
                            credits_text = credits_match.group(1)
                    # –ò—â–µ–º —Ü–µ–Ω—ã –≤ USD
                    if re.search(r'\$\s*\d+\.?\d*', text):
                        price_match = re.search(r'\$\s*(\d+\.?\d*)', text)
                        if price_match:
                            price_text = price_match.group(1)
                
                if model_cell and (credits_text or price_text):
                    models.append({
                        'model_name': model_cell,
                        'credits': float(credits_text) if credits_text else None,
                        'price_usd': float(price_text) if price_text else None
                    })
            except Exception as e:
                continue
    
    return models

def parse_api_docs_from_page(html, model_id):
    """–ü–∞—Ä—Å–∏—Ç—å API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–æ–¥–µ–ª–∏."""
    soup = BeautifulSoup(html, 'html.parser')
    
    model_info = {
        'model_id': model_id,
        'display_name': None,
        'category': None,
        'modality': None,
        'provider': None,
        'input_schema': {
            'required': [],
            'properties': {}
        },
        'pricing': {}
    }
    
    # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –º–æ–¥–µ–ª–∏
    h1 = soup.find('h1')
    if h1:
        model_info['display_name'] = h1.get_text(strip=True)
    
    # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    # –û–±—ã—á–Ω–æ –µ—Å—Ç—å —Ç–µ–≥–∏ –∏–ª–∏ badges
    badges = soup.find_all(['span', 'div'], class_=re.compile(r'badge|tag|label', re.I))
    for badge in badges:
        text = badge.get_text(strip=True).lower()
        if 'video' in text:
            model_info['category'] = 'video'
        elif 'image' in text:
            model_info['category'] = 'image'
        elif 'audio' in text or 'music' in text:
            model_info['category'] = 'audio'
    
    # –ò—â–µ–º JSON —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–æ–±—ã—á–Ω–æ –≤ script tag –∏–ª–∏ pre)
    scripts = soup.find_all('script')
    for script in scripts:
        if script.string:
            # –ò—â–µ–º JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            json_matches = re.findall(r'\{[^{}]*"model"[^{}]*\}', script.string)
            for match in json_matches:
                try:
                    data = json.loads(match)
                    if 'model' in data:
                        model_info['model_id'] = data['model']
                except:
                    pass
    
    return model_info

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    print("üöÄ KIE.AI MODEL SCRAPER")
    print("=" * 80)
    print()
    
    # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É pricing
    pricing_html = fetch_pricing_page()
    print(f"‚úÖ Pricing page loaded ({len(pricing_html)} bytes)")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    with open('/workspaces/5656/kie_pricing_full.html', 'w', encoding='utf-8') as f:
        f.write(pricing_html)
    print("üíæ Saved to kie_pricing_full.html")
    print()
    
    # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –º–æ–¥–µ–ª–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
    print("üìä Extracting models from pricing table...")
    models = extract_pricing_from_table(pricing_html)
    print(f"‚úÖ Found {len(models)} models in pricing table")
    print()
    
    # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏
    print("üîó Extracting model links...")
    model_links = extract_model_links(pricing_html)
    print(f"‚úÖ Found {len(model_links)} model links")
    print()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20 —Å—Å—ã–ª–æ–∫
    print("üìã First 20 model links:")
    for i, link in enumerate(model_links[:20], 1):
        print(f"  {i}. {link}")
    print()
    
    # 4. –î–ª—è –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏ –ø–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏ (–ª–∏–º–∏—Ç 10 –¥–ª—è –Ω–∞—á–∞–ª–∞)
    detailed_models = []
    
    print("üìñ Fetching model details (first 10)...")
    for i, link in enumerate(model_links[:10], 1):
        print(f"\n{i}/{min(10, len(model_links))}: {link}")
        
        model_html = fetch_model_page(link)
        if model_html:
            model_id = link.strip('/')
            model_info = parse_api_docs_from_page(model_html, model_id)
            detailed_models.append(model_info)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            safe_name = model_id.replace('/', '_')
            with open(f'/workspaces/5656/model_{safe_name}.html', 'w', encoding='utf-8') as f:
                f.write(model_html)
            print(f"  üíæ Saved HTML")
        
        time.sleep(0.5)  # Rate limiting
    
    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output = {
        'version': '5.1.0-scraped',
        'source': 'kie.ai/pricing (automated scraper)',
        'generated_at': '2024-12-24',
        'total_models_found': len(models),
        'total_links_found': len(model_links),
        'detailed_models_scraped': len(detailed_models),
        'models_from_table': models,
        'model_links': model_links,
        'detailed_models': detailed_models
    }
    
    with open('/workspaces/5656/models/kie_scraped_models.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print()
    print("=" * 80)
    print("‚úÖ SCRAPING COMPLETE")
    print(f"üìä Total models in pricing table: {len(models)}")
    print(f"üîó Total model links found: {len(model_links)}")
    print(f"üìñ Detailed models scraped: {len(detailed_models)}")
    print(f"üíæ Saved to models/kie_scraped_models.json")
    print()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä scraped –º–æ–¥–µ–ª–∏
    if detailed_models:
        print("üìã Example scraped model:")
        print(json.dumps(detailed_models[0], indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()
