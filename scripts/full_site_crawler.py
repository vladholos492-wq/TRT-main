#!/usr/bin/env python3
"""
–ü–æ–ª–Ω—ã–π –∫—Ä–∞—É–ª–µ—Ä –≤—Å–µ–≥–æ —Å–∞–π—Ç–∞ KIE AI.
–ü–∞—Ä—Å–∏—Ç –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –í–°–ï —Å—Å—ã–ª–∫–∏, –í–°–ï –±—É–∫–≤—ã, –í–°–Æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é.
"""

import os
import sys
import json
import asyncio
import logging
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
from datetime import datetime, timezone
from urllib.parse import urljoin, urlparse, urlunparse
from dotenv import load_dotenv

try:
    from playwright.async_api import async_playwright, Page, Browser, BrowserContext, Response
except ImportError:
    print("‚ùå playwright –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install playwright")
    print("   –ó–∞—Ç–µ–º: playwright install chromium")
    sys.exit(1)

root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –î–æ–º–µ–Ω—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
ALLOWED_DOMAINS = [
    'kie.ai',
    'www.kie.ai',
    'docs.kie.ai',
    'api.kie.ai'
]

# –ë–∞–∑–æ–≤—ã–µ URL
KIE_BASE_URL = "https://kie.ai"
KIE_DOCS_URL = "https://docs.kie.ai"

# –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
OUTPUT_DIR = root_dir / "data" / "kie_full_site"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# –ü–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
HTML_DIR = OUTPUT_DIR / "html"
TEXT_DIR = OUTPUT_DIR / "text"
JSON_DIR = OUTPUT_DIR / "json"
IMAGES_DIR = OUTPUT_DIR / "images"
LINKS_DIR = OUTPUT_DIR / "links"

for dir_path in [HTML_DIR, TEXT_DIR, JSON_DIR, IMAGES_DIR, LINKS_DIR]:
    dir_path.mkdir(exist_ok=True)


# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
STATS = {
    'total_pages': 0,
    'parsed_pages': 0,
    'failed_pages': 0,
    'total_links': 0,
    'unique_links': 0,
    'start_time': None,
    'end_time': None
}


class FullSiteCrawler:
    """–ü–æ–ª–Ω—ã–π –∫—Ä–∞—É–ª–µ—Ä —Å–∞–π—Ç–∞ KIE AI."""
    
    def __init__(self, headless: bool = True, max_depth: int = 10):
        self.headless = headless
        self.max_depth = max_depth
        
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        
        # –û—á–µ—Ä–µ–¥—å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.url_queue: List[Dict[str, Any]] = []
        self.visited_urls: Set[str] = set()
        self.url_index: Dict[str, Dict[str, Any]] = {}
        self.all_links: Set[str] = set()
        
        # –ù–∞—á–∞–ª—å–Ω—ã–µ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.start_urls = [
            "https://kie.ai",
            "https://kie.ai/market",
            "https://docs.kie.ai",
            "https://docs.kie.ai/market",
        ]
    
    async def __aenter__(self):
        """Async context manager entry."""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=['--no-sandbox', '--disable-setuid-sandbox']
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.browser:
            await self.browser.close()
    
    def is_allowed_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Ä–∞–∑—Ä–µ—à—ë–Ω –ª–∏ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞."""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
            if not any(allowed in domain for allowed in ALLOWED_DOMAINS):
                return False
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤
            excluded_extensions = ['.pdf', '.zip', '.exe', '.dmg', '.jpg', '.jpeg', '.png', '.gif', '.svg']
            path = parsed.path.lower()
            if any(path.endswith(ext) for ext in excluded_extensions):
                return False
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—É—Ç–∏
            excluded_paths = ['/api/', '/_next/', '/static/', '/assets/']
            if any(excluded in path for excluded in excluded_paths):
                return False
            
            return True
        except:
            return False
    
    def normalize_url(self, url: str, base_url: str = None) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL."""
        try:
            # –ï—Å–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL, –¥–µ–ª–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–º
            if not url.startswith('http'):
                if base_url:
                    url = urljoin(base_url, url)
                else:
                    return None
            
            parsed = urlparse(url)
            # –£–±–∏—Ä–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            normalized = urlunparse((
                parsed.scheme,
                parsed.netloc.lower(),
                parsed.path.rstrip('/'),
                '',
                '',
                ''
            ))
            
            return normalized
        except:
            return None
    
    def get_url_hash(self, url: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ö–µ—à URL –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞."""
        return hashlib.md5(url.encode()).hexdigest()
    
    
    async def extract_all_links(self, page: Page, current_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        links = []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
            link_elements = await page.query_selector_all('a[href]')
            
            for link in link_elements:
                try:
                    href = await link.get_attribute('href')
                    if not href:
                        continue
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                    normalized = self.normalize_url(href, current_url)
                    if normalized and self.is_allowed_url(normalized):
                        links.append(normalized)
                        self.all_links.add(normalized)
                except:
                    continue
            
            # –¢–∞–∫–∂–µ –∏—â–µ–º —Å—Å—ã–ª–∫–∏ –≤ JavaScript (data-–∞—Ç—Ä–∏–±—É—Ç—ã, onclick –∏ —Ç.–¥.)
            try:
                js_links = await page.evaluate("""
                    () => {
                        const links = [];
                        // –ò—â–µ–º –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
                        document.querySelectorAll('[data-href], [data-url], [data-link]').forEach(el => {
                            const href = el.getAttribute('data-href') || el.getAttribute('data-url') || el.getAttribute('data-link');
                            if (href) links.push(href);
                        });
                        return links;
                    }
                """)
                
                for href in js_links:
                    normalized = self.normalize_url(href, current_url)
                    if normalized and self.is_allowed_url(normalized):
                        links.append(normalized)
                        self.all_links.add(normalized)
            except:
                pass
            
            return list(set(links))
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫: {e}")
            return []
    
    async def parse_page(self, url: str, depth: int = 0) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É."""
        if depth > self.max_depth:
            return None
        
        if url in self.visited_urls:
            return self.url_index.get(url)
        
        normalized_url = self.normalize_url(url)
        if not normalized_url:
            return None
        
        if normalized_url in self.visited_urls:
            return self.url_index.get(normalized_url)
        
        self.visited_urls.add(normalized_url)
        STATS['total_pages'] += 1
        
        logger.info(f"üìÑ [{depth}] –ü–∞—Ä—Å–∏–Ω–≥: {normalized_url}")
        
        try:
            # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            page = await self.context.new_page()
            
            # –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º JSON –æ—Ç–≤–µ—Ç—ã
            json_responses = []
            
            async def handle_response(response: Response):
                try:
                    content_type = response.headers.get('content-type', '').lower()
                    if 'application/json' in content_type:
                        try:
                            json_data = await response.json()
                            json_responses.append({
                                'url': response.url,
                                'data': json_data
                            })
                        except:
                            pass
                except:
                    pass
            
            page.on('response', handle_response)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            await page.goto(normalized_url, wait_until='networkidle', timeout=30000)
            await asyncio.sleep(2)
            
            # –°–∫—Ä–æ–ª–ª–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(1)
            await page.evaluate("window.scrollTo(0, 0)")
            await asyncio.sleep(1)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            html_content = await page.content()
            text_content = await page.text_content('body') or ''
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
            links = await self.extract_all_links(page, normalized_url)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            url_hash = self.get_url_hash(normalized_url)
            
            page_data = {
                'url': normalized_url,
                'depth': depth,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'title': await page.title(),
                'html_length': len(html_content),
                'text_length': len(text_content),
                'links_count': len(links),
                'links': links,
                'json_responses_count': len(json_responses)
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML
            html_file = HTML_DIR / f"{url_hash}.html"
            html_file.write_text(html_content, encoding='utf-8')
            page_data['html_file'] = str(html_file.relative_to(OUTPUT_DIR))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç
            text_file = TEXT_DIR / f"{url_hash}.txt"
            text_file.write_text(text_content, encoding='utf-8')
            page_data['text_file'] = str(text_file.relative_to(OUTPUT_DIR))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON –æ—Ç–≤–µ—Ç—ã
            if json_responses:
                json_file = JSON_DIR / f"{url_hash}.json"
                json_file.write_text(
                    json.dumps(json_responses, ensure_ascii=False, indent=2),
                    encoding='utf-8'
                )
                page_data['json_file'] = str(json_file.relative_to(OUTPUT_DIR))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫–∏
            links_file = LINKS_DIR / f"{url_hash}_links.json"
            links_file.write_text(
                json.dumps(links, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
            
            self.url_index[normalized_url] = page_data
            STATS['parsed_pages'] += 1
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
            for link in links:
                if link not in self.visited_urls and self.is_allowed_url(link):
                    self.url_queue.append({
                        'url': link,
                        'depth': depth + 1
                    })
            
            await page.close()
            
            return page_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {normalized_url}: {e}")
            STATS['failed_pages'] += 1
            
            error_data = {
                'url': normalized_url,
                'depth': depth,
                'error': str(e),
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            self.url_index[normalized_url] = error_data
            return error_data
    
    async def crawl_all(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∫—Ä–∞—É–ª–∏–Ω–≥ –≤—Å–µ–≥–æ —Å–∞–π—Ç–∞."""
        STATS['start_time'] = datetime.now(timezone.utc).isoformat()
        logger.info("üöÄ –ù–∞—á–∞–ª–æ –ø–æ–ª–Ω–æ–≥–æ –∫—Ä–∞—É–ª–∏–Ω–≥–∞ —Å–∞–π—Ç–∞ KIE AI...")
        logger.info("‚ÑπÔ∏è  –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è - –ø–∞—Ä—Å–∏–º –ø—É–±–ª–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –±—Ä–∞—É–∑–µ—Ä–∞
        self.context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ URL –≤ –æ—á–µ—Ä–µ–¥—å
        for url in self.start_urls:
            self.url_queue.append({
                'url': url,
                'depth': 0
            })
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å, –µ—Å–ª–∏ –µ—Å—Ç—å
        progress_file = OUTPUT_DIR / "crawl_progress.json"
        if progress_file.exists():
            try:
                progress_data = json.loads(progress_file.read_text(encoding='utf-8'))
                self.visited_urls = set(progress_data.get('visited_urls', []))
                self.url_index = progress_data.get('url_index', {})
                self.all_links = set(progress_data.get('all_links', []))
                logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å: {len(self.visited_urls)} —Å—Ç—Ä–∞–Ω–∏—Ü —É–∂–µ —Å–ø–∞—Ä—Å–µ–Ω–æ")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å: {e}")
        
        # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –æ—á–µ—Ä–µ–¥–∏
        processed_count = 0
        while self.url_queue:
            item = self.url_queue.pop(0)
            url = item['url']
            depth = item['depth']
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —É–∂–µ –ø–æ—Å–µ—Ç–∏–ª–∏
            normalized = self.normalize_url(url)
            if normalized and normalized in self.visited_urls:
                continue
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            await self.parse_page(url, depth)
            processed_count += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü
            if processed_count % 10 == 0:
                try:
                    progress_data = {
                        'visited_urls': list(self.visited_urls),
                        'url_index': self.url_index,
                        'all_links': list(self.all_links),
                        'queue_size': len(self.url_queue),
                        'timestamp': datetime.now(timezone.utc).isoformat()
                    }
                    progress_file.write_text(
                        json.dumps(progress_data, ensure_ascii=False, indent=2),
                        encoding='utf-8'
                    )
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å: {e}")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(1)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if STATS['parsed_pages'] % 10 == 0:
                logger.info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {STATS['parsed_pages']} —Å—Ç—Ä–∞–Ω–∏—Ü, {len(self.url_queue)} –≤ –æ—á–µ—Ä–µ–¥–∏, {len(self.all_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
        
        STATS['end_time'] = datetime.now(timezone.utc).isoformat()
        STATS['unique_links'] = len(self.all_links)
        STATS['total_links'] = len(self.all_links)
        
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        progress_file = OUTPUT_DIR / "crawl_progress.json"
        if progress_file.exists():
            try:
                progress_file.unlink()
            except:
                pass
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
        index_file = OUTPUT_DIR / "site_index.json"
        index_data = {
            'stats': STATS,
            'url_index': self.url_index,
            'all_links': sorted(list(self.all_links)),
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        index_file.write_text(
            json.dumps(index_data, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )
        
        logger.info(f"‚úÖ –ö—Ä–∞—É–ª–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω!")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {STATS['total_pages']}")
        logger.info(f"   –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {STATS['parsed_pages']}")
        logger.info(f"   –û—à–∏–±–æ–∫: {STATS['failed_pages']}")
        logger.info(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {STATS['unique_links']}")
        logger.info(f"üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_DIR}")


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ü–æ–ª–Ω—ã–π –∫—Ä–∞—É–ª–µ—Ä —Å–∞–π—Ç–∞ KIE AI (–±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏)')
    parser.add_argument('--no-headless', action='store_true',
                        help='–ó–∞–ø—É—Å–∫ —Å –≤–∏–¥–∏–º—ã–º –±—Ä–∞—É–∑–µ—Ä–æ–º')
    parser.add_argument('--max-depth', type=int, default=10,
                        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ —Ä–µ–∫—É—Ä—Å–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
    
    args = parser.parse_args()
    
    headless = not args.no_headless
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∫—Ä–∞—É–ª–µ—Ä–∞ —Å–∞–π—Ç–∞ KIE AI")
    logger.info("‚ÑπÔ∏è  –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è - –ø–∞—Ä—Å–∏–º –ø—É–±–ª–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç")
    
    async with FullSiteCrawler(
        headless=headless,
        max_depth=args.max_depth
    ) as crawler:
        await crawler.crawl_all()
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

