#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–∏–Ω–≥ –í–°–ï–• —Ü–µ–Ω —Å https://kie.ai/pricing
–ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –ò–°–¢–û–ß–ù–ò–ö –ò–°–¢–ò–ù–´ –î–õ–Ø –¶–ï–ù
"""
import httpx
from bs4 import BeautifulSoup
import json
import re
from pathlib import Path


def scrape_pricing_page():
    """–ü–∞—Ä—Å–∏–Ω–≥ pricing page"""
    
    url = "https://kie.ai/pricing"
    
    print("=" * 80)
    print("üí∞ SCRAPING KIE.AI PRICING PAGE")
    print("=" * 80)
    print(f"\nüåê URL: {url}")
    
    try:
        with httpx.Client(timeout=30, follow_redirects=True) as client:
            resp = client.get(url)
            resp.raise_for_status()
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        cache_dir = Path('cache')
        cache_dir.mkdir(exist_ok=True)
        
        with open(cache_dir / 'kie_pricing_page.html', 'w', encoding='utf-8') as f:
            f.write(resp.text)
        
        print(f"‚úÖ HTML saved to cache/kie_pricing_page.html")
        
        # –ü–∞—Ä—Å–∏–º pricing —Ç–∞–±–ª–∏—Ü—É
        pricing_data = {}
        
        # –ò—â–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏ —Ü–µ–Ω—ã
        # –í–∞—Ä–∏–∞–Ω—Ç 1: –¢–∞–±–ª–∏—Ü—ã
        tables = soup.find_all('table')
        print(f"\nüìä Found {len(tables)} tables")
        
        for i, table in enumerate(tables):
            rows = table.find_all('tr')
            print(f"\n  Table {i+1}: {len(rows)} rows")
            
            for row in rows[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                cells = row.find_all(['td', 'th'])
                if cells:
                    text = ' | '.join(c.get_text(strip=True) for c in cells)
                    print(f"    {text[:100]}")
        
        # –í–∞—Ä–∏–∞–Ω—Ç 2: –ö–∞—Ä—Ç–æ—á–∫–∏/–±–ª–æ–∫–∏ —Å —Ü–µ–Ω–∞–º–∏
        # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å $ –∏–ª–∏ credits
        price_elements = soup.find_all(string=re.compile(r'\$\d+|\d+\s*credits?', re.IGNORECASE))
        print(f"\nüíµ Found {len(price_elements)} price mentions")
        
        for elem in price_elements[:10]:
            parent = elem.parent
            # –ò—â–µ–º model name —Ä—è–¥–æ–º
            siblings = parent.find_all(string=True)
            context = ' '.join(s.strip() for s in siblings if s.strip())
            print(f"  {context[:150]}")
        
        # –í–∞—Ä–∏–∞–Ω—Ç 3: JSON data –≤ script tags
        scripts = soup.find_all('script')
        print(f"\nüìú Found {len(scripts)} script tags")
        
        for script in scripts:
            if script.string and ('pricing' in script.string.lower() or 'models' in script.string.lower()):
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON
                content = script.string
                
                # –ò—â–µ–º JSON –æ–±—ä–µ–∫—Ç—ã
                json_matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content)
                
                for match in json_matches[:3]:
                    try:
                        data = json.loads(match)
                        if 'price' in str(data).lower() or 'model' in str(data).lower():
                            print(f"\n  Found JSON data: {str(data)[:200]}")
                    except:
                        pass
        
        # –í–∞—Ä–∏–∞–Ω—Ç 4: –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
        price_containers = soup.find_all(class_=re.compile(r'price|pricing|cost|model', re.IGNORECASE))
        print(f"\nüè∑Ô∏è  Found {len(price_containers)} elements with price-related classes")
        
        for elem in price_containers[:10]:
            print(f"  {elem.name}.{elem.get('class')}: {elem.get_text(strip=True)[:100]}")
        
        return pricing_data
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    pricing = scrape_pricing_page()
    
    if pricing is not None:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        output = Path('artifacts/pricing_from_page.json')
        with open(output, 'w', encoding='utf-8') as f:
            json.dump(pricing, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ Saved: {output}")
        print(f"   Models: {len(pricing)}")
    else:
        print("\n‚ö†Ô∏è  No pricing data extracted, check HTML manually")
        print("   File: cache/kie_pricing_page.html")


if __name__ == '__main__':
    main()
